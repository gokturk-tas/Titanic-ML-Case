# -*- coding: utf-8 -*-
"""titanic_case.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1XMbCELsb7hfXr8DM4ULbNbKCqhFKLELJ

# **Mastercard Case**

Arif Göktürk Taş


# What is supervised learning?
> Supervised learning is useful for classifying the data and predicting the outcomes of it. To predict the values it uses some parts of the data to train the model and other parts to test it if it's accurate.  

# What is unsupervised learning?
> Unsupervised learning is using algorithms for checking if there is any hidden pattern between values to cluster (group) the data. Since the data is unlabeled, after this operation a human or a supervised machine learning is needed to label the output data.

# What is the difference and which one should I use?
> Supervised learning is using labeled data which is the value that I am trying to predict. On the other hand unsupervised learning is using unlabeled data. In this case my label is the "Survived" column and since we have that value we should be using supervised learning.

> Other than that we I should be using classification models instead of regression models because I am trying to predict if the passenger is alive or not which is a state and not value.

# How do you Handle Outlier Values?
> To handle outlier values, first I took a look at the missing values. I noticed missing information on age, cabin and embarked columns. I was in between of two options. First one was deleting rows with missing values and second one was filling missing values with mode values. I decided to use second option because I didn't wan't to waste those informations. I also checked the outlier values of Fare column and removed the outlier values after looking boxplot of it.



# Informations I used during the case.
> 1. https://www.kaggle.com/c/titanic
2.   https://www.ibm.com/cloud/learn/unsupervised-learning
3.   https://www.ibm.com/cloud/learn/supervised-learning
4.   https://www.ibm.com/cloud/blog/supervised-vs-unsupervised-learning
5.   https://aws.amazon.com/tr/sagemaker/data-labeling/what-is-data-labeling/
6.   https://www.youtube.com/watch?v=rODWw2_1mCI
7.   https://elitedatascience.com/python-seaborn-tutorial
8.   https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.subplots.html
9.   https://towardsdatascience.com/master-the-art-of-subplots-in-python-45f7884f3d2e
10.  https://betterprogramming.pub/titanic-survival-prediction-using-machine-learning-4c5ff1e3fa16
11.  https://www.analyticsvidhya.com/blog/2021/04/difference-between-fit-transform-fit_transform-methods-in-scikit-learn-with-python-code/
12.  https://livebook.manning.com/book/grokking-machine-learning/2-1-what-is-the-difference-between-labelled-and-unlabelled-data-/v-4/1
13.  https://medium.com/analytics-vidhya/identifying-cleaning-and-replacing-outliers-titanic-dataset-20182a062893
14.  https://medium.com/analytics-vidhya/how-to-remove-outliers-for-machine-learning-24620c4657e8
15.  https://stackoverflow.com/questions/49139299/whisker-is-defined-as-1-5-iqr-how-could-two-whikers-in-plot-from-python-seabor
16.  https://www.analyticsvidhya.com/blog/2021/04/difference-between-fit-transform-fit_transform-methods-in-scikit-learn-with-python-code/
17.  https://indatalabs.com/blog/predictive-models-performance-evaluation-important
"""

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

#Get passengers data
url = 'https://raw.githubusercontent.com/gokturk-tas/mc_int/main/data.csv'
data = pd.read_csv(url)

data.describe()

data['Survived'].value_counts()

sns.countplot(data['Survived'])

cols = ['Sex', 'Pclass', 'SibSp', 'Parch', 'Embarked']

n_rows = 1
n_cols = 5
# The subplot grid and the figure size of each graph
# This returns a Figure (fig) and an Axes Object (axs)
fig, axs = plt.subplots(n_rows, n_cols, figsize=(n_cols*3.2,n_rows*3.2))

for r in range(0,n_rows):
    for c in range(0,n_cols):  
        
        i = r*n_cols+ c #index to go through the number of columns       
        ax = axs[c] #Show where to position each subplot
        sns.countplot(data[cols[i]], hue=data["Survived"], ax=ax)
        ax.set_title(cols[i])
        ax.legend(title="Survived", loc='upper right') 


plt.tight_layout()   #tight_layout

#Look at survival rate by sex
data.groupby('Sex')[['Survived']].mean()

#Look at survival rate by class
data.groupby('Pclass')[['Survived']].mean()

#Look at survival rate by sex and class
data.pivot_table('Survived', index='Sex', columns='Pclass')

data.pivot_table('Survived', index='Sex', columns='Pclass').plot()

#Plot the survival rate of each class.
sns.barplot(x='Pclass', y='Survived', data=data)

#Look at survival rate by sex, age and class
age = pd.cut(data['Age'], [0, 18, float('inf')])
data.pivot_table('Survived', ['Sex', age], 'Pclass')

#Prices Paid Of Each Class
plt.scatter(data['Fare'], data['Pclass'],  color = 'red', label='Person')
plt.ylabel('Class')
plt.xlabel('Ticket Price')
plt.title('Price Of Each Class')
plt.legend()
plt.show()

#Count the empty values in rows
data.isna().sum()

#Missin value percentage for colums with missing values
missing_values=data.isnull().sum()
missing_values[missing_values>0]/len(data)*100

# Removing irrelevant columns
data = data.drop(['PassengerId', 'Cabin', 'Name', 'Ticket'], axis=1)

#Remove the rows with missing values approach.
#data = data.dropna(subset =['Embarked', 'Age'])

#Filling the missing data with mode of that column
data['Age']=data['Age'].fillna(data['Age'].mode()[0])
data['Embarked']=data['Embarked'].fillna(data['Embarked'].mode()[0])

#Boxplot of fare value
sns.boxplot(data['Fare'],data=data)

data['Fare'].hist()
print('Skewness value of Age: ',data['Age'].skew())
print('Skewness value of Fare: ',data['Fare'].skew())

Q1 = data['Fare'].quantile(0.25)
Q3 = data['Fare'].quantile(0.75)
IQR = Q3 - Q1 #Interquartile range
whisker_width = 1.5

lower_whisker = Q1 - (whisker_width*IQR)
upper_whisker = Q3 + (whisker_width*IQR)

Fare_outliers = data[(data['Fare'] < lower_whisker) | (data['Fare'] > Q3 + upper_whisker)]
Fare_outliers.head()

data['Fare']=np.where(data['Fare']>upper_whisker,upper_whisker,np.where(data['Fare']<lower_whisker,lower_whisker,data['Fare']))
sns.boxplot(data['Fare'],data=data)

#Find the data types
data.dtypes

#Print the unique values in the Sex and Embarked
print(data['Sex'].unique())
print(data['Embarked'].unique())

from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()

#turn values to numeric only in sex column
data.iloc[:,2]= le.fit_transform(data.iloc[:,2].values)

#turn values to numeric only in embarked column
data.iloc[:,7] = le.fit_transform(data.iloc[:,7].values)

#Print the unique values in the columns
print(data['Sex'].unique())
print(data['Embarked'].unique())

data.dtypes

#Split the data into independent 'X' and dependent 'Y' variables
X = data.iloc[:, 1:8].values 
Y = data.iloc[:, 0].values 

# Split the dataset into 80% Training set and 20% Testing set
from sklearn.model_selection import train_test_split
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, random_state = 0)

#Feature Scaling
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)

#Create a function within many Machine Learning Models
def models(X_train,Y_train):
  
  #Using Logistic Regression Algorithm to the Training Set
  from sklearn.linear_model import LogisticRegression
  log = LogisticRegression(random_state = 0)
  log.fit(X_train, Y_train)
  
  #Using KNeighborsClassifier Method of neighbors class to use Nearest Neighbor algorithm
  from sklearn.neighbors import KNeighborsClassifier
  knn = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2)
  knn.fit(X_train, Y_train)

  #Using SVC method of svm class to use Support Vector Machine Algorithm
  from sklearn.svm import SVC
  svc_lin = SVC(kernel = 'linear', random_state = 0)
  svc_lin.fit(X_train, Y_train)

  #Using SVC method of svm class to use Kernel SVM Algorithm
  from sklearn.svm import SVC
  svc_rbf = SVC(kernel = 'rbf', random_state = 0)
  svc_rbf.fit(X_train, Y_train)

  #Using GaussianNB method of naïve_bayes class to use Naïve Bayes Algorithm
  from sklearn.naive_bayes import GaussianNB
  gauss = GaussianNB()
  gauss.fit(X_train, Y_train)

  #Using DecisionTreeClassifier of tree class to use Decision Tree Algorithm
  from sklearn.tree import DecisionTreeClassifier
  tree = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)
  tree.fit(X_train, Y_train)

  #Using RandomForestClassifier method of ensemble class to use Random Forest Classification algorithm
  from sklearn.ensemble import RandomForestClassifier
  forest = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0)
  forest.fit(X_train, Y_train)

  #Using Perceptron
  from sklearn.linear_model import Perceptron
  perceptron = Perceptron()
  perceptron.fit(X_train, Y_train)
  Y_pred = perceptron.predict(X_test)
  
  #print model accuracy on the training data.
  print('[0]Logistic Regression Training Accuracy:', log.score(X_train, Y_train))
  print('[1]K Nearest Neighbor Training Accuracy:', knn.score(X_train, Y_train))
  print('[2]Support Vector Machine (Linear Classifier) Training Accuracy:', svc_lin.score(X_train, Y_train))
  print('[3]Support Vector Machine (RBF Classifier) Training Accuracy:', svc_rbf.score(X_train, Y_train))
  print('[4]Gaussian Naive Bayes Training Accuracy:', gauss.score(X_train, Y_train))
  print('[5]Decision Tree Classifier Training Accuracy:', tree.score(X_train, Y_train))
  print('[6]Random Forest Classifier Training Accuracy:', forest.score(X_train, Y_train))
  print('[7]Perceptron Accuracy:', perceptron.score(X_train, Y_train))
  
  return log, knn, svc_lin, svc_rbf, gauss, tree, forest, perceptron

#Get and train all of the models
model = models(X_train,Y_train)

#TP FP
#FN TN
from sklearn.metrics import confusion_matrix 
for i in range(len(model)):
   cm = confusion_matrix(Y_test, model[i].predict(X_test)) 
   #extracting TN, FP, FN, TP
   TN, FP, FN, TP = confusion_matrix(Y_test, model[i].predict(X_test)).ravel()
   print(cm)
   print('Model[{}] Testing Accuracy = "{} !"'.format(i,  (TP + TN) / (TP + TN + FN + FP)))
   print()# Print a new line

#Get the importance of the features
forest = model[6]
importances = pd.DataFrame({'feature':data.iloc[:, 1:8].columns,'importance':np.round(forest.feature_importances_,3)})
importances = importances.sort_values('importance',ascending=False).set_index('feature')
importances

#Visualize the importance
importances.plot.bar()

pred = model[6].predict(X_test)
print("Prediction")
print(pred)

#Print a space
print()

#Print the actual values
print("Real Result")
print(Y_test)

class_test = 3
sex_test = 1
age_test = 20
sib_sp_test = 0
parents_test = 0
fare_test = 0
embarked_test = 0

test_survival = [[class_test,sex_test,age_test,sib_sp_test, parents_test, fare_test, embarked_test]]
test_survival = sc.transform(test_survival)
pred = model[6].predict(test_survival)

if pred == 0:
  print("Dead")
else:
  print('Alive')
